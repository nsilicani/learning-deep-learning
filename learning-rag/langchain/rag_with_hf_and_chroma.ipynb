{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJdag7X0ctUc"
      },
      "source": [
        "# Build RAG with Huggingface and Chroma\n",
        "IN this notebook we build a RAG pipeline with Langchain. We use Huggingface models for encoding and generation. The example is structured as follows:\n",
        "- [Load Data and Parameters](#load-data-and-params): Load PDF document (Bank of America FORM 10-q) and set parameters\n",
        "- [Chunking](#chunking): Chunk document using `RecursiveCharacterTextSplitter`\n",
        "- [Custom Embeddings](#custom-embeddings): Define a custom embedding strategy with `CLS pooling`\n",
        "- [Indexing](#indexing): Index document into Chroma DB\n",
        "- [Load Generative Model](#load-generative-model): Load generative model to answer query\n",
        "- [Set up LLM chain](#set-up-llm-chain): Set chain using `PromptTemplate`, `HuggingFacePipeline` and `StrOutputparser`\n",
        "- [Generation](#generation): Generate response based on retrieved context by leveraging generative model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wyygd1pGjbN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain langchain_community langchain-huggingface langchain-text-splitters langchain-chroma pypdf tqdm accelerate bitsandbytes python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xm1cbrNGV06"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmdU2OWLJCtJ"
      },
      "source": [
        "## Load Data and Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Btr-0JHAD0"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "if [ ! -f \"BAC_10Q.pdf\" ]; then\n",
        "    wget -q -O BAC_10Q.pdf https://investor.bankofamerica.com/regulatory-and-other-filings/all-sec-filings/content/0000070858-24-000208/0000070858-24-000208.pdf\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SviRgl_6Hy1l"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = pathlib.Path.cwd()\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "TOP_K = 4\n",
        "\n",
        "encoding_model = \"BAAI/bge-base-en-v1.5\"\n",
        "PADDING = True\n",
        "TRUNCATION = True\n",
        "\n",
        "generative_model = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "generation_kwargs = dict(\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=400,\n",
        ")\n",
        "\n",
        "if os.getenv(\"USE_COLAB_SECRET\", None):\n",
        "  from google.colab import userdata\n",
        "  os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3taI8WLJHYN"
      },
      "source": [
        "## Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbk9bos3H5op"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(DATA_PATH / \"BAC_10Q.pdf\")\n",
        "docs = loader.load()\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsIN5F6CH6x1"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "text_lines = [chunk.page_content for chunk in chunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5hRvuAFW2I-"
      },
      "source": [
        "## Custom Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjjga-NxW48V"
      },
      "outputs": [],
      "source": [
        "class CustomEmbeddings(Embeddings):\n",
        "    def __init__(self, model_name: str, tokenizer_kwargs: Dict[str, Any]):\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.model = AutoModel.from_pretrained(model_name)\n",
        "      self.tokenizer_kwargs = tokenizer_kwargs\n",
        "\n",
        "    def encode(self, text):\n",
        "      inputs = self.tokenizer(text, return_tensors=\"pt\", **self.tokenizer_kwargs)\n",
        "      with torch.no_grad():\n",
        "          # CLS Pooling\n",
        "          embeddings = self.model(**inputs).last_hidden_state[:, 0, :].cpu().numpy()\n",
        "      return embeddings\n",
        "\n",
        "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
        "        return [self.encode(d)[0].tolist() for d in documents]\n",
        "\n",
        "    def embed_query(self, query: str) -> List[float]:\n",
        "        return self.encode([query])[0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN3x-hQIYwYc"
      },
      "outputs": [],
      "source": [
        "custom_emb = CustomEmbeddings(model_name=encoding_model, tokenizer_kwargs={\"padding\": PADDING, \"truncation\": TRUNCATION})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ABf9ajOZZdz"
      },
      "source": [
        "## Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms6yxMWfaDjx"
      },
      "outputs": [],
      "source": [
        "vectorStore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    collection_name=\"dens_vecs_1\",\n",
        "    embedding=custom_emb,\n",
        "    persist_directory=\"./chroma_langchain_db\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT_35YWgZ-Nv"
      },
      "source": [
        "## Load Generative Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X-fTo4uaB45"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  text_generation_model = AutoModelForCausalLM.from_pretrained(generative_model, quantization_config=bnb_config)\n",
        "else:\n",
        "  text_generation_model = AutoModelForCausalLM.from_pretrained(generative_model)\n",
        "text_generation_tokenizer = AutoTokenizer.from_pretrained(generative_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnKNhFbwPndi"
      },
      "source": [
        "## Set up LLM chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s96Io4iCbxfG"
      },
      "outputs": [],
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=text_generation_model,\n",
        "    tokenizer=text_generation_tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    **generation_kwargs\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "<|system|>\n",
        "You are a smart assistant able to analyze companies' financial documents. Use the following pieces of information enclosed in <context> tags to provide an answer to the question. Return only the answer.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "<|user|>\n",
        "{question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=PROMPT,\n",
        ")\n",
        "\n",
        "llm_chain = prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9lWdA7_cho3"
      },
      "source": [
        "## Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNx_Ir-acq_g"
      },
      "outputs": [],
      "source": [
        "def create_context(results: List[Any]) -> str:\n",
        "    return \"\\n\\n\".join([res.page_content for res in results])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el9Tbp-JckbG"
      },
      "outputs": [],
      "source": [
        "query = \"What's the increase of Net income for Consumer Lending?\"\n",
        "results = vectorStore.similarity_search(query, k=TOP_K)\n",
        "context = create_context(results)\n",
        "rag_chain = {\"context\": lambda x: context, \"question\": RunnablePassthrough()} | llm_chain\n",
        "\n",
        "answer = rag_chain.invoke(query)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
